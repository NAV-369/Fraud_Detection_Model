{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature and Target Separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load datasets\n",
    "creditcard_df = pd.read_csv('../data/creditcard.csv')\n",
    "fraud_data_df = pd.read_csv('../data/Fraud_Data.csv')\n",
    "\n",
    "# Separate features and target for creditcard dataset\n",
    "X_creditcard = creditcard_df.drop(columns=['Class'])\n",
    "y_creditcard = creditcard_df['Class']\n",
    "\n",
    "# Separate features and target for fraud-data dataset\n",
    "X_fraud_data = fraud_data_df.drop(columns=['class'])\n",
    "y_fraud_data = fraud_data_df['class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Train-test split for creditcard dataset\n",
    "X_train_creditcard, X_test_creditcard, y_train_creditcard, y_test_creditcard = train_test_split(\n",
    "    X_creditcard, y_creditcard, test_size=0.2, random_state=42, stratify=y_creditcard\n",
    ")\n",
    "\n",
    "# Train-test split for fraud-data dataset\n",
    "X_train_fraud_data, X_test_fraud_data, y_train_fraud_data, y_test_fraud_data = train_test_split(\n",
    "    X_fraud_data, y_fraud_data, test_size=0.2, random_state=42, stratify=y_fraud_data\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We’ll use the following models for comparison:\n",
    "\n",
    "Logistic Regression\n",
    "\n",
    "Decision Tree\n",
    "\n",
    "Random Forest\n",
    "\n",
    "Gradient Boosting\n",
    "\n",
    "Multi-Layer Perceptron (MLP)\n",
    "\n",
    "Convolutional Neural Network (CNN)\n",
    "\n",
    "Recurrent Neural Network (RNN)\n",
    "\n",
    "Long Short-Term Memory (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56864\n",
      "           1       0.80      0.69      0.74        98\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.90      0.85      0.87     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n",
      "ROC AUC Score: 0.8467892960504404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zelalemtegene/envs/shared_env/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "# Train Logistic Regression\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(X_train_creditcard, y_train_creditcard)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_log_reg = log_reg.predict(X_test_creditcard)\n",
    "print(\"Logistic Regression Classification Report:\")\n",
    "print(classification_report(y_test_creditcard, y_pred_log_reg))\n",
    "print(\"ROC AUC Score:\", roc_auc_score(y_test_creditcard, y_pred_log_reg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56864\n",
      "           1       0.75      0.74      0.75        98\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.88      0.87      0.87     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Train Decision Tree\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "dt.fit(X_train_creditcard, y_train_creditcard)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_dt = dt.predict(X_test_creditcard)\n",
    "print(\"Decision Tree Classification Report:\")\n",
    "print(classification_report(y_test_creditcard, y_pred_dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56864\n",
      "           1       0.94      0.82      0.87        98\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.97      0.91      0.94     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Train Random Forest\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train_creditcard, y_train_creditcard)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_rf = rf.predict(X_test_creditcard)\n",
    "print(\"Random Forest Classification Report:\")\n",
    "print(classification_report(y_test_creditcard, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56864\n",
      "           1       0.53      0.18      0.27        98\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.76      0.59      0.64     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Train Gradient Boosting\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "gb.fit(X_train_creditcard, y_train_creditcard)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_gb = gb.predict(X_test_creditcard)\n",
    "print(\"Gradient Boosting Classification Report:\")\n",
    "print(classification_report(y_test_creditcard, y_pred_gb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Multi-Layer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56864\n",
      "           1       0.66      0.60      0.63        98\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.83      0.80      0.81     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Train MLP\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=1000, random_state=42)\n",
    "mlp.fit(X_train_creditcard, y_train_creditcard)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_mlp = mlp.predict(X_test_creditcard)\n",
    "print(\"MLP Classification Report:\")\n",
    "print(classification_report(y_test_creditcard, y_pred_mlp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zelalemtegene/envs/shared_env/lib/python3.9/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2025-02-11 05:48:06.107722: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1\n",
      "2025-02-11 05:48:06.107859: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2025-02-11 05:48:06.107865: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1739281686.108401  661633 pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1739281686.108845  661633 pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-11 05:48:08.431965: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5697/5697\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 15ms/step - accuracy: 0.9935 - loss: 14.0033 - val_accuracy: 0.9986 - val_loss: 1.7089\n",
      "Epoch 2/10\n",
      "\u001b[1m5697/5697\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 15ms/step - accuracy: 0.9984 - loss: 0.7076 - val_accuracy: 0.9993 - val_loss: 0.3049\n",
      "Epoch 3/10\n",
      "\u001b[1m5697/5697\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 14ms/step - accuracy: 0.9985 - loss: 0.6923 - val_accuracy: 0.9986 - val_loss: 1.7740\n",
      "Epoch 4/10\n",
      "\u001b[1m5697/5697\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 14ms/step - accuracy: 0.9986 - loss: 0.8377 - val_accuracy: 0.9991 - val_loss: 0.6648\n",
      "Epoch 5/10\n",
      "\u001b[1m5697/5697\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 14ms/step - accuracy: 0.9986 - loss: 0.8562 - val_accuracy: 0.9987 - val_loss: 1.6384\n",
      "Epoch 6/10\n",
      "\u001b[1m5697/5697\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 15ms/step - accuracy: 0.9981 - loss: 2.2394 - val_accuracy: 0.9987 - val_loss: 2.1733\n",
      "Epoch 7/10\n",
      "\u001b[1m5697/5697\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 15ms/step - accuracy: 0.9984 - loss: 1.8261 - val_accuracy: 0.9989 - val_loss: 1.7788\n",
      "Epoch 8/10\n",
      "\u001b[1m5697/5697\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 14ms/step - accuracy: 0.9984 - loss: 2.1176 - val_accuracy: 0.9990 - val_loss: 1.5437\n",
      "Epoch 9/10\n",
      "\u001b[1m5697/5697\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 14ms/step - accuracy: 0.9985 - loss: 2.5223 - val_accuracy: 0.9993 - val_loss: 1.4845\n",
      "Epoch 10/10\n",
      "\u001b[1m5697/5697\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 14ms/step - accuracy: 0.9985 - loss: 2.7162 - val_accuracy: 0.9990 - val_loss: 1.6742\n",
      "\u001b[1m1781/1781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "CNN Classification Report:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'classification_report' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m y_pred_cnn \u001b[38;5;241m=\u001b[39m (y_pred_cnn \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCNN Classification Report:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mclassification_report\u001b[49m(y_test_creditcard, y_pred_cnn))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'classification_report' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "\n",
    "# Reshape data for CNN\n",
    "X_train_creditcard_cnn = X_train_creditcard.values.reshape(X_train_creditcard.shape[0], X_train_creditcard.shape[1], 1)\n",
    "X_test_creditcard_cnn = X_test_creditcard.values.reshape(X_test_creditcard.shape[0], X_test_creditcard.shape[1], 1)\n",
    "\n",
    "# Build CNN model\n",
    "model_cnn = Sequential([\n",
    "    Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train_creditcard_cnn.shape[1], 1)),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Flatten(),\n",
    "    Dense(50, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_cnn.fit(X_train_creditcard_cnn, y_train_creditcard, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_cnn = model_cnn.predict(X_test_creditcard_cnn)\n",
    "y_pred_cnn = (y_pred_cnn > 0.5).astype(int)\n",
    "print(\"CNN Classification Report:\")\n",
    "print(classification_report(y_test_creditcard, y_pred_cnn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directly Run Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1781/1781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "CNN Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56864\n",
      "           1       0.63      0.80      0.71        98\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.82      0.90      0.85     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report  # Ensure import\n",
    "\n",
    "# Evaluate\n",
    "y_pred_cnn = model_cnn.predict(X_test_creditcard_cnn)\n",
    "y_pred_cnn = (y_pred_cnn > 0.5).astype(int)\n",
    "\n",
    "print(\"CNN Classification Report:\")\n",
    "print(classification_report(y_test_creditcard, y_pred_cnn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model after training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model_cnn.save(\"creditcard_fraud_cnn.h5\")  # Save model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Network (RNN) and LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zelalemtegene/envs/shared_env/lib/python3.9/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m5697/5697\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 17ms/step - accuracy: 0.9988 - loss: 0.0185 - val_accuracy: 0.9991 - val_loss: 0.0044\n",
      "Epoch 2/10\n",
      "\u001b[1m5697/5697\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 17ms/step - accuracy: 0.9993 - loss: 0.0039 - val_accuracy: 0.9994 - val_loss: 0.0039\n",
      "Epoch 3/10\n",
      "\u001b[1m5697/5697\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 17ms/step - accuracy: 0.9994 - loss: 0.0040 - val_accuracy: 0.9994 - val_loss: 0.0036\n",
      "Epoch 4/10\n",
      "\u001b[1m5697/5697\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 17ms/step - accuracy: 0.9993 - loss: 0.0034 - val_accuracy: 0.9993 - val_loss: 0.0043\n",
      "Epoch 5/10\n",
      "\u001b[1m5697/5697\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 17ms/step - accuracy: 0.9993 - loss: 0.0040 - val_accuracy: 0.9994 - val_loss: 0.0035\n",
      "Epoch 6/10\n",
      "\u001b[1m5697/5697\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 17ms/step - accuracy: 0.9993 - loss: 0.0039 - val_accuracy: 0.9994 - val_loss: 0.0038\n",
      "Epoch 7/10\n",
      "\u001b[1m5697/5697\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 17ms/step - accuracy: 0.9993 - loss: 0.0037 - val_accuracy: 0.9994 - val_loss: 0.0036\n",
      "Epoch 8/10\n",
      "\u001b[1m5697/5697\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 17ms/step - accuracy: 0.9994 - loss: 0.0035 - val_accuracy: 0.9993 - val_loss: 0.0037\n",
      "Epoch 9/10\n",
      "\u001b[1m5697/5697\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 17ms/step - accuracy: 0.9995 - loss: 0.0031 - val_accuracy: 0.9994 - val_loss: 0.0035\n",
      "Epoch 10/10\n",
      "\u001b[1m5697/5697\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 17ms/step - accuracy: 0.9995 - loss: 0.0028 - val_accuracy: 0.9994 - val_loss: 0.0034\n",
      "\u001b[1m1781/1781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step\n",
      "LSTM Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56864\n",
      "           1       0.80      0.82      0.81        98\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.90      0.91      0.90     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import LSTM, SimpleRNN\n",
    "\n",
    "# Reshape data for RNN/LSTM\n",
    "X_train_creditcard_rnn = X_train_creditcard.values.reshape(X_train_creditcard.shape[0], X_train_creditcard.shape[1], 1)\n",
    "X_test_creditcard_rnn = X_test_creditcard.values.reshape(X_test_creditcard.shape[0], X_test_creditcard.shape[1], 1)\n",
    "\n",
    "# Build LSTM model\n",
    "model_lstm = Sequential([\n",
    "    LSTM(50, input_shape=(X_train_creditcard_rnn.shape[1], 1)),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_lstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_lstm.fit(X_train_creditcard_rnn, y_train_creditcard, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_lstm = model_lstm.predict(X_test_creditcard_rnn)\n",
    "y_pred_lstm = (y_pred_lstm > 0.5).astype(int)\n",
    "print(\"LSTM Classification Report:\")\n",
    "print(classification_report(y_test_creditcard, y_pred_lstm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  SavedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm.save(\"lstm_model.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLOps Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Versioning and Experiment Tracking with MLflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h5/cqk4jd793hzd58k1ssq6mkd40000gn/T/ipykernel_73190/195552534.py:20: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  X[col] = pd.to_datetime(X[col]).astype(int) / 10**9  # Convert to UNIX timestamp\n",
      "/var/folders/h5/cqk4jd793hzd58k1ssq6mkd40000gn/T/ipykernel_73190/195552534.py:20: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  X[col] = pd.to_datetime(X[col]).astype(int) / 10**9  # Convert to UNIX timestamp\n",
      "/var/folders/h5/cqk4jd793hzd58k1ssq6mkd40000gn/T/ipykernel_73190/195552534.py:20: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  X[col] = pd.to_datetime(X[col]).astype(int) / 10**9  # Convert to UNIX timestamp\n",
      "/var/folders/h5/cqk4jd793hzd58k1ssq6mkd40000gn/T/ipykernel_73190/195552534.py:20: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  X[col] = pd.to_datetime(X[col]).astype(int) / 10**9  # Convert to UNIX timestamp\n",
      "/var/folders/h5/cqk4jd793hzd58k1ssq6mkd40000gn/T/ipykernel_73190/195552534.py:20: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  X[col] = pd.to_datetime(X[col]).astype(int) / 10**9  # Convert to UNIX timestamp\n",
      "/var/folders/h5/cqk4jd793hzd58k1ssq6mkd40000gn/T/ipykernel_73190/195552534.py:20: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  X[col] = pd.to_datetime(X[col]).astype(int) / 10**9  # Convert to UNIX timestamp\n",
      "/var/folders/h5/cqk4jd793hzd58k1ssq6mkd40000gn/T/ipykernel_73190/195552534.py:20: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  X[col] = pd.to_datetime(X[col]).astype(int) / 10**9  # Convert to UNIX timestamp\n",
      "/var/folders/h5/cqk4jd793hzd58k1ssq6mkd40000gn/T/ipykernel_73190/195552534.py:20: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  X[col] = pd.to_datetime(X[col]).astype(int) / 10**9  # Convert to UNIX timestamp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Credit Card Model Trained. Accuracy: 0.9991\n",
      "\n",
      "Logistic Regression (Fraud_Data) Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      1.00      0.95     27393\n",
      "           1       0.00      0.00      0.00      2830\n",
      "\n",
      "    accuracy                           0.91     30223\n",
      "   macro avg       0.45      0.50      0.48     30223\n",
      "weighted avg       0.82      0.91      0.86     30223\n",
      "\n",
      "ROC AUC Score: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zelalemtegene/envs/shared_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/zelalemtegene/envs/shared_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/zelalemtegene/envs/shared_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
    "\n",
    "# Ensure datasets are defined before running the script\n",
    "# Example placeholders (Replace these with actual datasets)\n",
    "# X_train_creditcard, X_test_creditcard, y_train_creditcard, y_test_creditcard = ...\n",
    "# X_train_fraud_data, X_test_fraud_data, y_train_fraud_data, y_test_fraud_data = ...\n",
    "\n",
    "# Function to preprocess datasets\n",
    "def preprocess_data(X):\n",
    "    # Convert datetime columns to numeric timestamps\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        for col in X.select_dtypes(include=[\"datetime\", \"object\"]).columns:\n",
    "            try:\n",
    "                X[col] = pd.to_datetime(X[col]).astype(int) / 10**9  # Convert to UNIX timestamp\n",
    "            except:\n",
    "                X.drop(columns=[col], inplace=True)  # Drop non-convertible object columns\n",
    "    return X\n",
    "\n",
    "# Apply preprocessing\n",
    "X_train_creditcard = preprocess_data(X_train_creditcard)\n",
    "X_test_creditcard = preprocess_data(X_test_creditcard)\n",
    "X_train_fraud_data = preprocess_data(X_train_fraud_data)\n",
    "X_test_fraud_data = preprocess_data(X_test_fraud_data)\n",
    "\n",
    "# Standardizing datasets\n",
    "scaler_credit = StandardScaler()\n",
    "X_train_creditcard = scaler_credit.fit_transform(X_train_creditcard)\n",
    "X_test_creditcard = scaler_credit.transform(X_test_creditcard)\n",
    "\n",
    "scaler_fraud = StandardScaler()\n",
    "X_train_fraud_data = scaler_fraud.fit_transform(X_train_fraud_data)\n",
    "X_test_fraud_data = scaler_fraud.transform(X_test_fraud_data)\n",
    "\n",
    "### 📌 Train and Log Logistic Regression for Credit Card Data\n",
    "log_reg_credit = LogisticRegression(max_iter=500)  # Increased max_iter\n",
    "\n",
    "with mlflow.start_run(run_name=\"LogReg_CreditCard\"):\n",
    "    mlflow.log_param(\"model\", \"Logistic Regression - Credit Card\")\n",
    "    \n",
    "    log_reg_credit.fit(X_train_creditcard, y_train_creditcard)\n",
    "    y_pred_credit = log_reg_credit.predict(X_test_creditcard)\n",
    "    accuracy_credit = accuracy_score(y_test_creditcard, y_pred_credit)\n",
    "    \n",
    "    mlflow.log_metric(\"accuracy\", accuracy_credit)\n",
    "    \n",
    "    # Log model with input example to avoid MLflow warning\n",
    "    input_example_credit = np.expand_dims(X_test_creditcard[0], axis=0)\n",
    "    mlflow.sklearn.log_model(log_reg_credit, \"logistic_regression_credit_model\", input_example=input_example_credit)\n",
    "\n",
    "print(f\"Credit Card Model Trained. Accuracy: {accuracy_credit:.4f}\")\n",
    "\n",
    "### 📌 Train and Log Logistic Regression for Fraud Detection Data\n",
    "log_reg_fraud = LogisticRegression(max_iter=1000)  # Increased max_iter\n",
    "\n",
    "with mlflow.start_run(run_name=\"LogReg_FraudData\"):\n",
    "    mlflow.log_param(\"model\", \"Logistic Regression - Fraud Data\")\n",
    "    \n",
    "    log_reg_fraud.fit(X_train_fraud_data, y_train_fraud_data)\n",
    "    y_pred_fraud = log_reg_fraud.predict(X_test_fraud_data)\n",
    "    \n",
    "    # Compute performance metrics\n",
    "    accuracy_fraud = accuracy_score(y_test_fraud_data, y_pred_fraud)\n",
    "    roc_auc_fraud = roc_auc_score(y_test_fraud_data, y_pred_fraud)\n",
    "    \n",
    "    mlflow.log_metric(\"accuracy\", accuracy_fraud)\n",
    "    mlflow.log_metric(\"roc_auc\", roc_auc_fraud)\n",
    "\n",
    "    # Log model with input example\n",
    "    input_example_fraud = np.expand_dims(X_test_fraud_data[0], axis=0)\n",
    "    mlflow.sklearn.log_model(log_reg_fraud, \"logistic_regression_fraud_model\", input_example=input_example_fraud)\n",
    "\n",
    "print(\"\\nLogistic Regression (Fraud_Data) Classification Report:\")\n",
    "print(classification_report(y_test_fraud_data, y_pred_fraud))\n",
    "print(f\"ROC AUC Score: {roc_auc_fraud:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate All Models for creditcard Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Define the evaluate_model function\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    return {\n",
    "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"Precision\": precision_score(y_test, y_pred),\n",
    "        \"Recall\": recall_score(y_test, y_pred),\n",
    "        \"F1-Score\": f1_score(y_test, y_pred),\n",
    "        \"ROC-AUC\": roc_auc_score(y_test, y_pred)\n",
    "    }\n",
    "\n",
    "# Evaluate all models for creditcard dataset\n",
    "performance_summary_creditcard = {\n",
    "    \"Logistic Regression\": evaluate_model(log_reg, X_test_creditcard, y_test_creditcard),\n",
    "    \"Decision Tree\": evaluate_model(dt, X_test_creditcard, y_test_creditcard),\n",
    "    \"Random Forest\": evaluate_model(rf, X_test_creditcard, y_test_creditcard),\n",
    "    \"Gradient Boosting\": evaluate_model(gb, X_test_creditcard, y_test_creditcard),\n",
    "    \"MLP\": evaluate_model(mlp, X_test_creditcard, y_test_creditcard),\n",
    "    \"CNN\": evaluate_model(model_cnn, X_test_creditcard_cnn, y_test_creditcard),\n",
    "    \"LSTM\": evaluate_model(model_lstm, X_test_creditcard_rnn, y_test_creditcard)\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "performance_df_creditcard = pd.DataFrame(performance_summary_creditcard).T\n",
    "print(\"Performance Summary (creditcard):\")\n",
    "print(performance_df_creditcard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate All Models for Fraud_Data Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all models for Fraud_Data dataset\n",
    "performance_summary_fraud_data = {\n",
    "    \"Logistic Regression\": evaluate_model(log_reg_fraud, X_test_fraud_data, y_test_fraud_data),\n",
    "    \"Decision Tree\": evaluate_model(dt_fraud, X_test_fraud_data, y_test_fraud_data),\n",
    "    \"Random Forest\": evaluate_model(rf_fraud, X_test_fraud_data, y_test_fraud_data),\n",
    "    \"Gradient Boosting\": evaluate_model(gb_fraud, X_test_fraud_data, y_test_fraud_data),\n",
    "    \"MLP\": evaluate_model(mlp_fraud, X_test_fraud_data, y_test_fraud_data),\n",
    "    \"CNN\": evaluate_model(model_cnn_fraud, X_test_fraud_data_cnn, y_test_fraud_data),\n",
    "    \"LSTM\": evaluate_model(model_lstm_fraud, X_test_fraud_data_rnn, y_test_fraud_data)\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "performance_df_fraud_data = pd.DataFrame(performance_summary_fraud_data).T\n",
    "print(\"Performance Summary (Fraud_Data):\")\n",
    "print(performance_df_fraud_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Combine Results for Both Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column to indicate the dataset\n",
    "performance_df_creditcard['Dataset'] = 'creditcard'\n",
    "performance_df_fraud_data['Dataset'] = 'Fraud_Data'\n",
    "\n",
    "# Combine the results\n",
    "combined_performance_df = pd.concat([performance_df_creditcard, performance_df_fraud_data])\n",
    "print(\"Combined Performance Summary:\")\n",
    "print(combined_performance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set up the plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=combined_performance_df.index, y=combined_performance_df['ROC-AUC'], hue=combined_performance_df['Dataset'])\n",
    "plt.title(\"ROC-AUC Scores for All Models (Creditcard vs Fraud_Data)\")\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"ROC-AUC\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use MLflow to Track Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "# Log Logistic Regression experiment for creditcard dataset\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_param(\"dataset\", \"creditcard\")\n",
    "    mlflow.log_param(\"model\", \"Logistic Regression\")\n",
    "    log_reg.fit(X_train_creditcard, y_train_creditcard)\n",
    "    metrics = evaluate_model(log_reg, X_test_creditcard, y_test_creditcard)\n",
    "    for metric_name, metric_value in metrics.items():\n",
    "        mlflow.log_metric(metric_name, metric_value)\n",
    "    mlflow.sklearn.log_model(log_reg, \"logistic_regression_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Define a Function to Log Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "def log_experiment(model, X_train, y_train, X_test, y_test, dataset_name, model_name):\n",
    "    \"\"\"\n",
    "    Logs an experiment using MLflow.\n",
    "    \"\"\"\n",
    "    with mlflow.start_run():\n",
    "        # Log parameters\n",
    "        mlflow.log_param(\"dataset\", dataset_name)\n",
    "        mlflow.log_param(\"model\", model_name)\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluate the model\n",
    "        y_pred = model.predict(X_test)\n",
    "        metrics = {\n",
    "            \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "            \"Precision\": precision_score(y_test, y_pred),\n",
    "            \"Recall\": recall_score(y_test, y_pred),\n",
    "            \"F1-Score\": f1_score(y_test, y_pred),\n",
    "            \"ROC-AUC\": roc_auc_score(y_test, y_pred)\n",
    "        }\n",
    "        \n",
    "        # Log metrics\n",
    "        for metric_name, metric_value in metrics.items():\n",
    "            mlflow.log_metric(metric_name, metric_value)\n",
    "        \n",
    "        # Log the model\n",
    "        mlflow.sklearn.log_model(model, model_name)\n",
    "        \n",
    "        print(f\"Logged {model_name} for {dataset_name} dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Models and Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"MLP\": MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=1000, random_state=42)\n",
    "}\n",
    "\n",
    "# Define datasets\n",
    "datasets = {\n",
    "    \"creditcard\": (X_train_creditcard, y_train_creditcard, X_test_creditcard, y_test_creditcard),\n",
    "    \"Fraud_Data\": (X_train_fraud_data, y_train_fraud_data, X_test_fraud_data, y_test_fraud_data)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Experiments for All Models and Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log experiments for all models and datasets\n",
    "for dataset_name, (X_train, y_train, X_test, y_test) in datasets.items():\n",
    "    for model_name, model in models.items():\n",
    "        log_experiment(model, X_train, y_train, X_test, y_test, dataset_name, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Experiments for Deep Learning Models (CNN, RNN, LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow.tensorflow\n",
    "\n",
    "def log_dl_experiment(model, X_train, y_train, X_test, y_test, dataset_name, model_name):\n",
    "    \"\"\"\n",
    "    Logs a deep learning experiment using MLflow.\n",
    "    \"\"\"\n",
    "    with mlflow.start_run():\n",
    "        # Log parameters\n",
    "        mlflow.log_param(\"dataset\", dataset_name)\n",
    "        mlflow.log_param(\"model\", model_name)\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "        \n",
    "        # Evaluate the model\n",
    "        y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "        metrics = {\n",
    "            \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "            \"Precision\": precision_score(y_test, y_pred),\n",
    "            \"Recall\": recall_score(y_test, y_pred),\n",
    "            \"F1-Score\": f1_score(y_test, y_pred),\n",
    "            \"ROC-AUC\": roc_auc_score(y_test, y_pred)\n",
    "        }\n",
    "        \n",
    "        # Log metrics\n",
    "        for metric_name, metric_value in metrics.items():\n",
    "            mlflow.log_metric(metric_name, metric_value)\n",
    "        \n",
    "        # Log the model\n",
    "        mlflow.tensorflow.log_model(model, model_name)\n",
    "        \n",
    "        print(f\"Logged {model_name} for {dataset_name} dataset.\")\n",
    "\n",
    "# Log CNN, RNN, and LSTM experiments\n",
    "dl_models = {\n",
    "    \"CNN\": model_cnn,\n",
    "    \"RNN\": model_rnn,\n",
    "    \"LSTM\": model_lstm\n",
    "}\n",
    "\n",
    "for dataset_name, (X_train, y_train, X_test, y_test) in datasets.items():\n",
    "    for model_name, model in dl_models.items():\n",
    "        log_dl_experiment(model, X_train, y_train, X_test, y_test, dataset_name, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Logged Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bash: mlflow ui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Performance Summary Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a Function to Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluates a model and returns a dictionary of metrics.\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    return {\n",
    "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"Precision\": precision_score(y_test, y_pred),\n",
    "        \"Recall\": recall_score(y_test, y_pred),\n",
    "        \"F1-Score\": f1_score(y_test, y_pred),\n",
    "        \"ROC-AUC\": roc_auc_score(y_test, y_pred)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate All Models for Both Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"MLP\": MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=1000, random_state=42)\n",
    "}\n",
    "\n",
    "# Define datasets\n",
    "datasets = {\n",
    "    \"creditcard\": (X_train_creditcard, y_train_creditcard, X_test_creditcard, y_test_creditcard),\n",
    "    \"Fraud_Data\": (X_train_fraud_data, y_train_fraud_data, X_test_fraud_data, y_test_fraud_data)\n",
    "}\n",
    "\n",
    "# Initialize a dictionary to store performance results\n",
    "performance_summary = {}\n",
    "\n",
    "# Evaluate all models for both datasets\n",
    "for dataset_name, (X_train, y_train, X_test, y_test) in datasets.items():\n",
    "    for model_name, model in models.items():\n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluate the model\n",
    "        metrics = evaluate_model(model, X_test, y_test)\n",
    "        \n",
    "        # Store results in the performance_summary dictionary\n",
    "        performance_summary[f\"{dataset_name} - {model_name}\"] = metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the Results into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert the performance summary dictionary to a DataFrame\n",
    "performance_df = pd.DataFrame(performance_summary).T\n",
    "\n",
    "# Add a column for the dataset and model names\n",
    "performance_df.reset_index(inplace=True)\n",
    "performance_df.rename(columns={\"index\": \"Dataset - Model\"}, inplace=True)\n",
    "\n",
    "# Display the performance summary table\n",
    "print(\"Performance Summary Table:\")\n",
    "print(performance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Deep Learning Models (CNN, RNN, LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define deep learning models\n",
    "dl_models = {\n",
    "    \"CNN\": model_cnn,\n",
    "    \"RNN\": model_rnn,\n",
    "    \"LSTM\": model_lstm\n",
    "}\n",
    "\n",
    "# Evaluate deep learning models for both datasets\n",
    "for dataset_name, (X_train, y_train, X_test, y_test) in datasets.items():\n",
    "    for model_name, model in dl_models.items():\n",
    "        # Evaluate the model\n",
    "        y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "        metrics = {\n",
    "            \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "            \"Precision\": precision_score(y_test, y_pred),\n",
    "            \"Recall\": recall_score(y_test, y_pred),\n",
    "            \"F1-Score\": f1_score(y_test, y_pred),\n",
    "            \"ROC-AUC\": roc_auc_score(y_test, y_pred)\n",
    "        }\n",
    "        \n",
    "        # Store results in the performance_summary dictionary\n",
    "        performance_summary[f\"{dataset_name} - {model_name}\"] = metrics\n",
    "\n",
    "# Convert the updated performance summary dictionary to a DataFrame\n",
    "performance_df = pd.DataFrame(performance_summary).T\n",
    "performance_df.reset_index(inplace=True)\n",
    "performance_df.rename(columns={\"index\": \"Dataset - Model\"}, inplace=True)\n",
    "\n",
    "# Display the updated performance summary table\n",
    "print(\"Updated Performance Summary Table (Including Deep Learning Models):\")\n",
    "print(performance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Performance Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the performance summary table to a CSV file\n",
    "performance_df.to_csv(\"performance_summary.csv\", index=False)\n",
    "print(\"Performance summary saved to 'performance_summary.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set up the plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=performance_df[\"Dataset - Model\"], y=performance_df[\"ROC-AUC\"])\n",
    "plt.title(\"ROC-AUC Scores for All Models (Creditcard vs Fraud_Data)\")\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"ROC-AUC\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
