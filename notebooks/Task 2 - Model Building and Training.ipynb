{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature and Target Separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load datasets\n",
    "creditcard_df = pd.read_csv('../data/creditcard.csv')\n",
    "fraud_data_df = pd.read_csv('../data/Fraud_Data.csv')\n",
    "\n",
    "# Separate features and target for creditcard dataset\n",
    "X_creditcard = creditcard_df.drop(columns=['Class'])\n",
    "y_creditcard = creditcard_df['Class']\n",
    "\n",
    "# Separate features and target for fraud-data dataset\n",
    "X_fraud_data = fraud_data_df.drop(columns=['class'])\n",
    "y_fraud_data = fraud_data_df['class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Train-test split for creditcard dataset\n",
    "X_train_creditcard, X_test_creditcard, y_train_creditcard, y_test_creditcard = train_test_split(\n",
    "    X_creditcard, y_creditcard, test_size=0.2, random_state=42, stratify=y_creditcard\n",
    ")\n",
    "\n",
    "# Train-test split for fraud-data dataset\n",
    "X_train_fraud_data, X_test_fraud_data, y_train_fraud_data, y_test_fraud_data = train_test_split(\n",
    "    X_fraud_data, y_fraud_data, test_size=0.2, random_state=42, stratify=y_fraud_data\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weâ€™ll use the following models for comparison:\n",
    "\n",
    "Logistic Regression\n",
    "\n",
    "Decision Tree\n",
    "\n",
    "Random Forest\n",
    "\n",
    "Gradient Boosting\n",
    "\n",
    "Multi-Layer Perceptron (MLP)\n",
    "\n",
    "Convolutional Neural Network (CNN)\n",
    "\n",
    "Recurrent Neural Network (RNN)\n",
    "\n",
    "Long Short-Term Memory (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56864\n",
      "           1       0.80      0.69      0.74        98\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.90      0.85      0.87     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n",
      "ROC AUC Score: 0.8467892960504404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zelalemtegene/envs/shared_env/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "# Train Logistic Regression\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(X_train_creditcard, y_train_creditcard)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_log_reg = log_reg.predict(X_test_creditcard)\n",
    "print(\"Logistic Regression Classification Report:\")\n",
    "print(classification_report(y_test_creditcard, y_pred_log_reg))\n",
    "print(\"ROC AUC Score:\", roc_auc_score(y_test_creditcard, y_pred_log_reg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56864\n",
      "           1       0.75      0.74      0.75        98\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.88      0.87      0.87     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Train Decision Tree\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "dt.fit(X_train_creditcard, y_train_creditcard)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_dt = dt.predict(X_test_creditcard)\n",
    "print(\"Decision Tree Classification Report:\")\n",
    "print(classification_report(y_test_creditcard, y_pred_dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56864\n",
      "           1       0.94      0.82      0.87        98\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.97      0.91      0.94     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Train Random Forest\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train_creditcard, y_train_creditcard)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_rf = rf.predict(X_test_creditcard)\n",
    "print(\"Random Forest Classification Report:\")\n",
    "print(classification_report(y_test_creditcard, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56864\n",
      "           1       0.53      0.18      0.27        98\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.76      0.59      0.64     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Train Gradient Boosting\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "gb.fit(X_train_creditcard, y_train_creditcard)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_gb = gb.predict(X_test_creditcard)\n",
    "print(\"Gradient Boosting Classification Report:\")\n",
    "print(classification_report(y_test_creditcard, y_pred_gb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Multi-Layer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56864\n",
      "           1       0.66      0.60      0.63        98\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.83      0.80      0.81     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Train MLP\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=1000, random_state=42)\n",
    "mlp.fit(X_train_creditcard, y_train_creditcard)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_mlp = mlp.predict(X_test_creditcard)\n",
    "print(\"MLP Classification Report:\")\n",
    "print(classification_report(y_test_creditcard, y_pred_mlp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "\n",
    "# Reshape data for CNN\n",
    "X_train_creditcard_cnn = X_train_creditcard.values.reshape(X_train_creditcard.shape[0], X_train_creditcard.shape[1], 1)\n",
    "X_test_creditcard_cnn = X_test_creditcard.values.reshape(X_test_creditcard.shape[0], X_test_creditcard.shape[1], 1)\n",
    "\n",
    "# Build CNN model\n",
    "model_cnn = Sequential([\n",
    "    Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train_creditcard_cnn.shape[1], 1)),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Flatten(),\n",
    "    Dense(50, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_cnn.fit(X_train_creditcard_cnn, y_train_creditcard, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_cnn = model_cnn.predict(X_test_creditcard_cnn)\n",
    "y_pred_cnn = (y_pred_cnn > 0.5).astype(int)\n",
    "print(\"CNN Classification Report:\")\n",
    "print(classification_report(y_test_creditcard, y_pred_cnn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Network (RNN) and LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM, SimpleRNN\n",
    "\n",
    "# Reshape data for RNN/LSTM\n",
    "X_train_creditcard_rnn = X_train_creditcard.values.reshape(X_train_creditcard.shape[0], X_train_creditcard.shape[1], 1)\n",
    "X_test_creditcard_rnn = X_test_creditcard.values.reshape(X_test_creditcard.shape[0], X_test_creditcard.shape[1], 1)\n",
    "\n",
    "# Build LSTM model\n",
    "model_lstm = Sequential([\n",
    "    LSTM(50, input_shape=(X_train_creditcard_rnn.shape[1], 1)),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_lstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_lstm.fit(X_train_creditcard_rnn, y_train_creditcard, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_lstm = model_lstm.predict(X_test_creditcard_rnn)\n",
    "y_pred_lstm = (y_pred_lstm > 0.5).astype(int)\n",
    "print(\"LSTM Classification Report:\")\n",
    "print(classification_report(y_test_creditcard, y_pred_lstm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLOps Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Versioning and Experiment Tracking with MLflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_creditcard' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m mlflow\u001b[38;5;241m.\u001b[39mstart_run():\n\u001b[1;32m     10\u001b[0m     mlflow\u001b[38;5;241m.\u001b[39mlog_param(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLogistic Regression\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m     log_reg\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_train_creditcard\u001b[49m, y_train_creditcard)  \u001b[38;5;66;03m# Ensure X_train_creditcard and y_train_creditcard are defined\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m log_reg\u001b[38;5;241m.\u001b[39mpredict(X_test_creditcard)\n\u001b[1;32m     14\u001b[0m     accuracy \u001b[38;5;241m=\u001b[39m log_reg\u001b[38;5;241m.\u001b[39mscore(X_test_creditcard, y_test_creditcard)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train_creditcard' is not defined"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define and train the model\n",
    "log_reg = LogisticRegression()\n",
    "\n",
    "# Example: Logging Logistic Regression\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_param(\"model\", \"Logistic Regression\")\n",
    "    \n",
    "    log_reg.fit(X_train_creditcard, y_train_creditcard)  # Ensure X_train_creditcard and y_train_creditcard are defined\n",
    "    y_pred = log_reg.predict(X_test_creditcard)\n",
    "    accuracy = log_reg.score(X_test_creditcard, y_test_creditcard)\n",
    "    \n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    mlflow.sklearn.log_model(log_reg, \"logistic_regression_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Logistic Regression on Fraud_Data\n",
    "log_reg_fraud = LogisticRegression(max_iter=1000)\n",
    "log_reg_fraud.fit(X_train_fraud_data, y_train_fraud_data)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_log_reg_fraud = log_reg_fraud.predict(X_test_fraud_data)\n",
    "print(\"Logistic Regression (Fraud_Data) Classification Report:\")\n",
    "print(classification_report(y_test_fraud_data, y_pred_log_reg_fraud))\n",
    "print(\"ROC AUC Score:\", roc_auc_score(y_test_fraud_data, y_pred_log_reg_fraud))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate All Models for creditcard Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Define the evaluate_model function\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    return {\n",
    "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"Precision\": precision_score(y_test, y_pred),\n",
    "        \"Recall\": recall_score(y_test, y_pred),\n",
    "        \"F1-Score\": f1_score(y_test, y_pred),\n",
    "        \"ROC-AUC\": roc_auc_score(y_test, y_pred)\n",
    "    }\n",
    "\n",
    "# Evaluate all models for creditcard dataset\n",
    "performance_summary_creditcard = {\n",
    "    \"Logistic Regression\": evaluate_model(log_reg, X_test_creditcard, y_test_creditcard),\n",
    "    \"Decision Tree\": evaluate_model(dt, X_test_creditcard, y_test_creditcard),\n",
    "    \"Random Forest\": evaluate_model(rf, X_test_creditcard, y_test_creditcard),\n",
    "    \"Gradient Boosting\": evaluate_model(gb, X_test_creditcard, y_test_creditcard),\n",
    "    \"MLP\": evaluate_model(mlp, X_test_creditcard, y_test_creditcard),\n",
    "    \"CNN\": evaluate_model(model_cnn, X_test_creditcard_cnn, y_test_creditcard),\n",
    "    \"LSTM\": evaluate_model(model_lstm, X_test_creditcard_rnn, y_test_creditcard)\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "performance_df_creditcard = pd.DataFrame(performance_summary_creditcard).T\n",
    "print(\"Performance Summary (creditcard):\")\n",
    "print(performance_df_creditcard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate All Models for Fraud_Data Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all models for Fraud_Data dataset\n",
    "performance_summary_fraud_data = {\n",
    "    \"Logistic Regression\": evaluate_model(log_reg_fraud, X_test_fraud_data, y_test_fraud_data),\n",
    "    \"Decision Tree\": evaluate_model(dt_fraud, X_test_fraud_data, y_test_fraud_data),\n",
    "    \"Random Forest\": evaluate_model(rf_fraud, X_test_fraud_data, y_test_fraud_data),\n",
    "    \"Gradient Boosting\": evaluate_model(gb_fraud, X_test_fraud_data, y_test_fraud_data),\n",
    "    \"MLP\": evaluate_model(mlp_fraud, X_test_fraud_data, y_test_fraud_data),\n",
    "    \"CNN\": evaluate_model(model_cnn_fraud, X_test_fraud_data_cnn, y_test_fraud_data),\n",
    "    \"LSTM\": evaluate_model(model_lstm_fraud, X_test_fraud_data_rnn, y_test_fraud_data)\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "performance_df_fraud_data = pd.DataFrame(performance_summary_fraud_data).T\n",
    "print(\"Performance Summary (Fraud_Data):\")\n",
    "print(performance_df_fraud_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Combine Results for Both Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column to indicate the dataset\n",
    "performance_df_creditcard['Dataset'] = 'creditcard'\n",
    "performance_df_fraud_data['Dataset'] = 'Fraud_Data'\n",
    "\n",
    "# Combine the results\n",
    "combined_performance_df = pd.concat([performance_df_creditcard, performance_df_fraud_data])\n",
    "print(\"Combined Performance Summary:\")\n",
    "print(combined_performance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set up the plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=combined_performance_df.index, y=combined_performance_df['ROC-AUC'], hue=combined_performance_df['Dataset'])\n",
    "plt.title(\"ROC-AUC Scores for All Models (Creditcard vs Fraud_Data)\")\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"ROC-AUC\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use MLflow to Track Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "# Log Logistic Regression experiment for creditcard dataset\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_param(\"dataset\", \"creditcard\")\n",
    "    mlflow.log_param(\"model\", \"Logistic Regression\")\n",
    "    log_reg.fit(X_train_creditcard, y_train_creditcard)\n",
    "    metrics = evaluate_model(log_reg, X_test_creditcard, y_test_creditcard)\n",
    "    for metric_name, metric_value in metrics.items():\n",
    "        mlflow.log_metric(metric_name, metric_value)\n",
    "    mlflow.sklearn.log_model(log_reg, \"logistic_regression_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Define a Function to Log Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "def log_experiment(model, X_train, y_train, X_test, y_test, dataset_name, model_name):\n",
    "    \"\"\"\n",
    "    Logs an experiment using MLflow.\n",
    "    \"\"\"\n",
    "    with mlflow.start_run():\n",
    "        # Log parameters\n",
    "        mlflow.log_param(\"dataset\", dataset_name)\n",
    "        mlflow.log_param(\"model\", model_name)\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluate the model\n",
    "        y_pred = model.predict(X_test)\n",
    "        metrics = {\n",
    "            \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "            \"Precision\": precision_score(y_test, y_pred),\n",
    "            \"Recall\": recall_score(y_test, y_pred),\n",
    "            \"F1-Score\": f1_score(y_test, y_pred),\n",
    "            \"ROC-AUC\": roc_auc_score(y_test, y_pred)\n",
    "        }\n",
    "        \n",
    "        # Log metrics\n",
    "        for metric_name, metric_value in metrics.items():\n",
    "            mlflow.log_metric(metric_name, metric_value)\n",
    "        \n",
    "        # Log the model\n",
    "        mlflow.sklearn.log_model(model, model_name)\n",
    "        \n",
    "        print(f\"Logged {model_name} for {dataset_name} dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Models and Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"MLP\": MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=1000, random_state=42)\n",
    "}\n",
    "\n",
    "# Define datasets\n",
    "datasets = {\n",
    "    \"creditcard\": (X_train_creditcard, y_train_creditcard, X_test_creditcard, y_test_creditcard),\n",
    "    \"Fraud_Data\": (X_train_fraud_data, y_train_fraud_data, X_test_fraud_data, y_test_fraud_data)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Experiments for All Models and Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log experiments for all models and datasets\n",
    "for dataset_name, (X_train, y_train, X_test, y_test) in datasets.items():\n",
    "    for model_name, model in models.items():\n",
    "        log_experiment(model, X_train, y_train, X_test, y_test, dataset_name, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Experiments for Deep Learning Models (CNN, RNN, LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow.tensorflow\n",
    "\n",
    "def log_dl_experiment(model, X_train, y_train, X_test, y_test, dataset_name, model_name):\n",
    "    \"\"\"\n",
    "    Logs a deep learning experiment using MLflow.\n",
    "    \"\"\"\n",
    "    with mlflow.start_run():\n",
    "        # Log parameters\n",
    "        mlflow.log_param(\"dataset\", dataset_name)\n",
    "        mlflow.log_param(\"model\", model_name)\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "        \n",
    "        # Evaluate the model\n",
    "        y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "        metrics = {\n",
    "            \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "            \"Precision\": precision_score(y_test, y_pred),\n",
    "            \"Recall\": recall_score(y_test, y_pred),\n",
    "            \"F1-Score\": f1_score(y_test, y_pred),\n",
    "            \"ROC-AUC\": roc_auc_score(y_test, y_pred)\n",
    "        }\n",
    "        \n",
    "        # Log metrics\n",
    "        for metric_name, metric_value in metrics.items():\n",
    "            mlflow.log_metric(metric_name, metric_value)\n",
    "        \n",
    "        # Log the model\n",
    "        mlflow.tensorflow.log_model(model, model_name)\n",
    "        \n",
    "        print(f\"Logged {model_name} for {dataset_name} dataset.\")\n",
    "\n",
    "# Log CNN, RNN, and LSTM experiments\n",
    "dl_models = {\n",
    "    \"CNN\": model_cnn,\n",
    "    \"RNN\": model_rnn,\n",
    "    \"LSTM\": model_lstm\n",
    "}\n",
    "\n",
    "for dataset_name, (X_train, y_train, X_test, y_test) in datasets.items():\n",
    "    for model_name, model in dl_models.items():\n",
    "        log_dl_experiment(model, X_train, y_train, X_test, y_test, dataset_name, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Logged Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bash: mlflow ui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Performance Summary Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a Function to Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluates a model and returns a dictionary of metrics.\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    return {\n",
    "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"Precision\": precision_score(y_test, y_pred),\n",
    "        \"Recall\": recall_score(y_test, y_pred),\n",
    "        \"F1-Score\": f1_score(y_test, y_pred),\n",
    "        \"ROC-AUC\": roc_auc_score(y_test, y_pred)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate All Models for Both Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"MLP\": MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=1000, random_state=42)\n",
    "}\n",
    "\n",
    "# Define datasets\n",
    "datasets = {\n",
    "    \"creditcard\": (X_train_creditcard, y_train_creditcard, X_test_creditcard, y_test_creditcard),\n",
    "    \"Fraud_Data\": (X_train_fraud_data, y_train_fraud_data, X_test_fraud_data, y_test_fraud_data)\n",
    "}\n",
    "\n",
    "# Initialize a dictionary to store performance results\n",
    "performance_summary = {}\n",
    "\n",
    "# Evaluate all models for both datasets\n",
    "for dataset_name, (X_train, y_train, X_test, y_test) in datasets.items():\n",
    "    for model_name, model in models.items():\n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluate the model\n",
    "        metrics = evaluate_model(model, X_test, y_test)\n",
    "        \n",
    "        # Store results in the performance_summary dictionary\n",
    "        performance_summary[f\"{dataset_name} - {model_name}\"] = metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the Results into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert the performance summary dictionary to a DataFrame\n",
    "performance_df = pd.DataFrame(performance_summary).T\n",
    "\n",
    "# Add a column for the dataset and model names\n",
    "performance_df.reset_index(inplace=True)\n",
    "performance_df.rename(columns={\"index\": \"Dataset - Model\"}, inplace=True)\n",
    "\n",
    "# Display the performance summary table\n",
    "print(\"Performance Summary Table:\")\n",
    "print(performance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Deep Learning Models (CNN, RNN, LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define deep learning models\n",
    "dl_models = {\n",
    "    \"CNN\": model_cnn,\n",
    "    \"RNN\": model_rnn,\n",
    "    \"LSTM\": model_lstm\n",
    "}\n",
    "\n",
    "# Evaluate deep learning models for both datasets\n",
    "for dataset_name, (X_train, y_train, X_test, y_test) in datasets.items():\n",
    "    for model_name, model in dl_models.items():\n",
    "        # Evaluate the model\n",
    "        y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "        metrics = {\n",
    "            \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "            \"Precision\": precision_score(y_test, y_pred),\n",
    "            \"Recall\": recall_score(y_test, y_pred),\n",
    "            \"F1-Score\": f1_score(y_test, y_pred),\n",
    "            \"ROC-AUC\": roc_auc_score(y_test, y_pred)\n",
    "        }\n",
    "        \n",
    "        # Store results in the performance_summary dictionary\n",
    "        performance_summary[f\"{dataset_name} - {model_name}\"] = metrics\n",
    "\n",
    "# Convert the updated performance summary dictionary to a DataFrame\n",
    "performance_df = pd.DataFrame(performance_summary).T\n",
    "performance_df.reset_index(inplace=True)\n",
    "performance_df.rename(columns={\"index\": \"Dataset - Model\"}, inplace=True)\n",
    "\n",
    "# Display the updated performance summary table\n",
    "print(\"Updated Performance Summary Table (Including Deep Learning Models):\")\n",
    "print(performance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Performance Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the performance summary table to a CSV file\n",
    "performance_df.to_csv(\"performance_summary.csv\", index=False)\n",
    "print(\"Performance summary saved to 'performance_summary.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set up the plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=performance_df[\"Dataset - Model\"], y=performance_df[\"ROC-AUC\"])\n",
    "plt.title(\"ROC-AUC Scores for All Models (Creditcard vs Fraud_Data)\")\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"ROC-AUC\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
